{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-16 00:19:19.731964: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2023-10-16 00:19:19.732016: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2023-10-16 00:19:19.732036: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-10-16 00:19:19.737566: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from fastapi import FastAPI, File, UploadFile\n",
    "from fastapi.middleware.cors import CORSMiddleware\n",
    "import uvicorn\n",
    "import numpy as np\n",
    "from io import BytesIO\n",
    "from PIL import Image\n",
    "import tensorflow as tf\n",
    "import asyncio\n",
    "import nest_asyncio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "absl-py==2.0.0\n",
      "annotated-types==0.6.0\n",
      "anyio==3.7.1\n",
      "asttokens==2.4.0\n",
      "astunparse==1.6.3\n",
      "backcall==0.2.0\n",
      "cachetools==5.3.1\n",
      "certifi==2023.7.22\n",
      "charset-normalizer==3.3.0\n",
      "click==8.1.7\n",
      "comm==0.1.4\n",
      "contourpy==1.1.1\n",
      "cycler==0.12.1\n",
      "debugpy==1.8.0\n",
      "decorator==5.1.1\n",
      "dm-tree==0.1.8\n",
      "exceptiongroup==1.1.3\n",
      "executing==2.0.0\n",
      "fastapi==0.103.2\n",
      "flatbuffers==23.5.26\n",
      "fonttools==4.43.1\n",
      "gast==0.5.4\n",
      "google-auth==2.23.3\n",
      "google-auth-oauthlib==1.0.0\n",
      "google-pasta==0.2.0\n",
      "grpcio==1.59.0\n",
      "h11==0.14.0\n",
      "h5py==3.10.0\n",
      "httptools==0.6.0\n",
      "idna==3.4\n",
      "ipykernel==6.25.2\n",
      "ipython==8.16.1\n",
      "jedi==0.19.1\n",
      "jupyter_client==8.3.1\n",
      "jupyter_core==5.4.0\n",
      "keras==2.14.0\n",
      "keras-core==0.1.7\n",
      "keras-tuner==1.4.4\n",
      "kiwisolver==1.4.5\n",
      "kt-legacy==1.0.5\n",
      "libclang==16.0.6\n",
      "Markdown==3.5\n",
      "markdown-it-py==3.0.0\n",
      "MarkupSafe==2.1.3\n",
      "matplotlib==3.8.0\n",
      "matplotlib-inline==0.1.6\n",
      "mdurl==0.1.2\n",
      "ml-dtypes==0.2.0\n",
      "namex==0.0.7\n",
      "nest-asyncio==1.5.8\n",
      "numpy==1.26.0\n",
      "nvidia-cublas-cu11==11.11.3.6\n",
      "nvidia-cuda-cupti-cu11==11.8.87\n",
      "nvidia-cuda-nvcc-cu11==11.8.89\n",
      "nvidia-cuda-runtime-cu11==11.8.89\n",
      "nvidia-cudnn-cu11==8.7.0.84\n",
      "nvidia-cufft-cu11==10.9.0.58\n",
      "nvidia-curand-cu11==10.3.0.86\n",
      "nvidia-cusolver-cu11==11.4.1.48\n",
      "nvidia-cusparse-cu11==11.7.5.86\n",
      "nvidia-nccl-cu11==2.16.5\n",
      "oauthlib==3.2.2\n",
      "opt-einsum==3.3.0\n",
      "packaging==23.2\n",
      "parso==0.8.3\n",
      "pexpect==4.8.0\n",
      "pickleshare==0.7.5\n",
      "Pillow==10.0.1\n",
      "platformdirs==3.11.0\n",
      "prompt-toolkit==3.0.39\n",
      "protobuf==4.24.4\n",
      "psutil==5.9.5\n",
      "ptyprocess==0.7.0\n",
      "pure-eval==0.2.2\n",
      "pyasn1==0.5.0\n",
      "pyasn1-modules==0.3.0\n",
      "pydantic==2.4.2\n",
      "pydantic_core==2.10.1\n",
      "Pygments==2.16.1\n",
      "pyparsing==3.1.1\n",
      "python-dateutil==2.8.2\n",
      "python-dotenv==1.0.0\n",
      "python-multipart==0.0.6\n",
      "PyYAML==6.0.1\n",
      "pyzmq==25.1.1\n",
      "requests==2.31.0\n",
      "requests-oauthlib==1.3.1\n",
      "rich==13.6.0\n",
      "rsa==4.9\n",
      "six==1.16.0\n",
      "sniffio==1.3.0\n",
      "stack-data==0.6.3\n",
      "starlette==0.27.0\n",
      "tensorboard==2.14.1\n",
      "tensorboard-data-server==0.7.1\n",
      "tensorflow==2.14.0\n",
      "tensorflow-estimator==2.14.0\n",
      "tensorflow-io-gcs-filesystem==0.34.0\n",
      "tensorrt==8.5.3.1\n",
      "termcolor==2.3.0\n",
      "tornado==6.3.3\n",
      "traitlets==5.11.2\n",
      "typing_extensions==4.8.0\n",
      "urllib3==2.0.6\n",
      "uvicorn==0.23.2\n",
      "uvloop==0.17.0\n",
      "watchfiles==0.20.0\n",
      "wcwidth==0.2.8\n",
      "websockets==11.0.3\n",
      "Werkzeug==3.0.0\n",
      "wrapt==1.14.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "#pip freeze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Google Cloud Storage Client Library Version: 2.12.0\n"
     ]
    }
   ],
   "source": [
    "# import google.cloud.storage\n",
    "\n",
    "# # Get the version of the google-cloud-storage library\n",
    "# storage_version = google.cloud.storage.__version__\n",
    "\n",
    "# print(f\"Google Cloud Storage Client Library Version: {storage_version}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-16 00:20:13.192277: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:880] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-10-16 00:20:13.438531: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:880] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-10-16 00:20:13.438567: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:880] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-10-16 00:20:13.440371: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:880] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-10-16 00:20:13.440398: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:880] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-10-16 00:20:13.440415: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:880] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-10-16 00:20:17.123069: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:880] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-10-16 00:20:17.123113: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:880] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-10-16 00:20:17.123121: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1977] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2023-10-16 00:20:17.123299: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:880] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-10-16 00:20:17.123320: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1886] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 2475 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 960, pci bus id: 0000:01:00.0, compute capability: 5.2\n",
      "INFO:     Started server process [667]\n",
      "INFO:     Waiting for application startup.\n",
      "INFO:     Application startup complete.\n",
      "INFO:     Uvicorn running on http://localhost:8000 (Press CTRL+C to quit)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     127.0.0.1:45878 - \"GET / HTTP/1.1\" 404 Not Found\n",
      "INFO:     127.0.0.1:45878 - \"GET /favicon.ico HTTP/1.1\" 404 Not Found\n",
      "INFO:     127.0.0.1:45870 - \"GET /ping HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-16 00:21:01.381442: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:442] Loaded cuDNN version 8700\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 14s 14s/step\n",
      "INFO:     127.0.0.1:34564 - \"POST /predict HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:     Shutting down\n",
      "INFO:     Waiting for application shutdown.\n",
      "INFO:     Application shutdown complete.\n",
      "INFO:     Finished server process [667]\n"
     ]
    }
   ],
   "source": [
    "nest_asyncio.apply()\n",
    "async def my_coroutine():\n",
    "    app = FastAPI()\n",
    "\n",
    "    origins = [\n",
    "    \"http://localhost/\",\n",
    "    \"http://localhost:3000/\",\n",
    "]\n",
    "    app.add_middleware(\n",
    "        CORSMiddleware,\n",
    "        allow_origins=origins,\n",
    "        allow_credentials=True,\n",
    "        allow_methods=[\"\"],\n",
    "        allow_headers=[\"\"],\n",
    "    )\n",
    "\n",
    "    Model = tf.keras.models.load_model(\"../saved_models1/1\")\n",
    "\n",
    "    Class_names = [\"Tomato_Bacterial_spot\", \"Tomato_Early_blight\", \"Tomato_Late_blight\",\"Tomato_Leaf_Mold\",\n",
    "                    \"Tomato_Septoria_leaf_spot\",\"Tomato__Target_Spot\",\"Tomato__Tomato_YellowLeaf__Curl_Virus\",\"Tomato__Tomato_mosaic_virus\",\"Tomato_healthy\"]\n",
    "\n",
    "    @app.get(\"/ping\")\n",
    "    async def ping():\n",
    "        return \"Hello, I am alive\"\n",
    "    \n",
    "\n",
    "    def read_file_as_image(data) -> np.ndarray:\n",
    "        image=np.array(Image.open(BytesIO(data)))\n",
    "        return image\n",
    "\n",
    "    @app.post(\"/predict\")\n",
    "    async def predict(\n",
    "        file: UploadFile = File(...)\n",
    "        ):\n",
    "        image = read_file_as_image(await file.read())\n",
    "        img_batch = np.expand_dims(image,0)\n",
    "\n",
    "        prediction = Model.predict(img_batch)\n",
    "\n",
    "        predicted_class = Class_names[np.argmax(prediction[0])]\n",
    "        confidence=np.max(prediction[0])\n",
    "        \n",
    "        return{\n",
    "            'class' : predicted_class,\n",
    "            'confidence': float(confidence)\n",
    "        }\n",
    "    \n",
    "    if __name__ == \"__main__\":\n",
    "        uvicorn.run(app, host='localhost', port=8000)\n",
    "\n",
    "loop = asyncio.get_event_loop()\n",
    "loop.run_until_complete(my_coroutine())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Asi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
